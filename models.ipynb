{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wfdb\n",
    "from scipy import signal\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path to PTBXL, please adjust accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = '/Users/aroraji/Desktop/PersonalProjects/ML/GermanyProject/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(DATASET_PATH, 'ptbxl_database.csv'), index_col='ecg_id')\n",
    "scp_df = pd.read_csv(os.path.join(DATASET_PATH, 'scp_statements.csv'), index_col=0)\n",
    "df['scp_codes'] = df['scp_codes'].apply(lambda x: eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_diagnostic(y_dic):\n",
    "    tmp = []\n",
    "    for key in y_dic.keys():\n",
    "        if key in scp_df.index:\n",
    "            diagnostic_class = scp_df.loc[key]['diagnostic_class']\n",
    "            if isinstance(diagnostic_class, str) and diagnostic_class.strip():\n",
    "                tmp.append(diagnostic_class)\n",
    "    return list(set(tmp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['diagnostic_superclass'] = df['scp_codes'].apply(aggregate_diagnostic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ecg_id\n",
      "1    [NORM]\n",
      "2    [NORM]\n",
      "3    [NORM]\n",
      "4    [NORM]\n",
      "5    [NORM]\n",
      "Name: diagnostic_superclass, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['diagnostic_superclass'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['diagnostic_superclass'].map(len) > 0].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0           [NORM]\n",
      "1           [NORM]\n",
      "2           [NORM]\n",
      "3           [NORM]\n",
      "4           [NORM]\n",
      "          ...     \n",
      "995    [HYP, STTC]\n",
      "996         [NORM]\n",
      "997         [NORM]\n",
      "998         [NORM]\n",
      "999           [CD]\n",
      "Name: diagnostic_superclass, Length: 1000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['diagnostic_superclass'].head(1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_subdiagnostic(y_dic):\n",
    "    tmp = []\n",
    "    for key in y_dic.keys():\n",
    "        if key in scp_df.index:\n",
    "            diagnostic_subclass = scp_df.loc[key]['diagnostic_subclass']\n",
    "            if isinstance(diagnostic_subclass, str) and diagnostic_subclass.strip():\n",
    "                tmp.append(diagnostic_subclass)\n",
    "    return list(set(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['diagnostic_subclass'] = df['scp_codes'].apply(aggregate_subdiagnostic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0           [NORM]\n",
      "1           [NORM]\n",
      "2           [NORM]\n",
      "3           [NORM]\n",
      "4           [NORM]\n",
      "          ...     \n",
      "995    [LVH, STTC]\n",
      "996         [NORM]\n",
      "997         [NORM]\n",
      "998         [NORM]\n",
      "999        [CRBBB]\n",
      "Name: diagnostic_subclass, Length: 1000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['diagnostic_subclass'].head(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refining the data and End Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['diagnostic_superclass'] = df['diagnostic_superclass'].apply(\n",
    "    lambda lst: [x for x in lst if isinstance(x, str) and x.strip()]\n",
    ")\n",
    "df = df[df['diagnostic_superclass'].map(len) > 0].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['diagnostic_subclass'] = df['diagnostic_subclass'].apply(\n",
    "    lambda lst: [x for x in lst if isinstance(x, str) and x.strip()]\n",
    ")\n",
    "df = df[df['diagnostic_subclass'].map(len) > 0].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique diagnostic classes:\n",
      "['STTC' 'NORM' 'MI' 'HYP' 'CD' nan]\n"
     ]
    }
   ],
   "source": [
    "scp_df = pd.read_csv(\n",
    "    os.path.join(DATASET_PATH, 'scp_statements.csv'),\n",
    "    index_col=0\n",
    ")\n",
    "print(\"Unique diagnostic classes:\")\n",
    "print(scp_df['diagnostic_class'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [NORM]\n",
      "1    [NORM]\n",
      "2    [NORM]\n",
      "3    [NORM]\n",
      "4    [NORM]\n",
      "Name: diagnostic_superclass, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['diagnostic_superclass'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0           [NORM]\n",
      "1           [NORM]\n",
      "2           [NORM]\n",
      "3           [NORM]\n",
      "4           [NORM]\n",
      "          ...     \n",
      "995    [LVH, STTC]\n",
      "996         [NORM]\n",
      "997         [NORM]\n",
      "998         [NORM]\n",
      "999        [CRBBB]\n",
      "Name: diagnostic_subclass, Length: 1000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['diagnostic_subclass'].head(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a Multilabel Binarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Classes:\n",
      "['AMI' 'CD' 'CLBBB' 'CRBBB' 'HYP' 'ILBBB' 'IMI' 'IRBBB' 'ISCA' 'ISCI'\n",
      " 'ISC_' 'IVCD' 'LAFB/LPFB' 'LAO/LAE' 'LMI' 'LVH' 'MI' 'NORM' 'NST_' 'PMI'\n",
      " 'RAO/RAE' 'RVH' 'SEHYP' 'STTC' 'WPW' '_AVB']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "all_labels = df['diagnostic_superclass'].tolist() + df['diagnostic_subclass'].tolist()\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(all_labels)\n",
    "labels = mlb.transform(df['diagnostic_superclass'])  # For initial labeling\n",
    "\n",
    "print(\"All Classes:\")\n",
    "print(mlb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['filename'] = df['filename_hr'].apply(lambda x: os.path.join(DATASET_PATH, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continued Preprocessing of Signals (Plus Downsampling to reduce Computations for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_signals(df):\n",
    "    signals = []\n",
    "    for filename in df['filename']:\n",
    "        signal, fields = wfdb.rdsamp(filename)\n",
    "        signals.append(signal)\n",
    "    return np.array(signals)\n",
    "\n",
    "X = load_signals(df)\n",
    "\n",
    "def downsample_signals(X, target_length=1000):\n",
    "    X_downsampled = signal.resample(X, target_length, axis=1)\n",
    "    return X_downsampled\n",
    "\n",
    "X = downsample_signals(X, target_length=1000)\n",
    "\n",
    "def normalize_signals(X):\n",
    "    N, L, C = X.shape\n",
    "    X_normalized = np.zeros_like(X)\n",
    "    for i in range(N):\n",
    "        scaler = StandardScaler()\n",
    "        X_normalized[i] = scaler.fit_transform(X[i])\n",
    "    return X_normalized\n",
    "\n",
    "X = normalize_signals(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()\n",
    "train_indices = df[df.strat_fold < 10].index\n",
    "test_indices = df[df.strat_fold == 10].index\n",
    "\n",
    "X_train = X[train_indices]\n",
    "X_test = X[test_indices]\n",
    "\n",
    "y_train = labels[train_indices]\n",
    "y_test = labels[test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X.shape[1:]\n",
    "num_classes = len(mlb_all.classes_)\n",
    "\n",
    "def build_resnet(input_shape, num_classes):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv1D(64, kernel_size=7, strides=2, padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.MaxPooling1D(pool_size=3, strides=2, padding='same')(x)\n",
    "\n",
    "    def res_block(x, filters, kernel_size=3, stride=1):\n",
    "        shortcut = x\n",
    "        x = layers.Conv1D(filters, kernel_size, strides=stride, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.ReLU()(x)\n",
    "        x = layers.Conv1D(filters, kernel_size, strides=1, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        if stride != 1 or x.shape[-1] != shortcut.shape[-1]:\n",
    "            shortcut = layers.Conv1D(filters, kernel_size=1, strides=stride, padding='same')(shortcut)\n",
    "            shortcut = layers.BatchNormalization()(shortcut)\n",
    "        x = layers.add([x, shortcut])\n",
    "        x = layers.ReLU()(x)\n",
    "        return x\n",
    "\n",
    "    x = res_block(x, 64)\n",
    "    x = res_block(x, 64)\n",
    "    \n",
    "    x = res_block(x, 128, stride=2)\n",
    "    x = res_block(x, 128)\n",
    "    \n",
    "    x = res_block(x, 256, stride=2)\n",
    "    x = res_block(x, 256)\n",
    "    \n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    outputs = layers.Dense(num_classes, activation='sigmoid')(x)\n",
    "    \n",
    "    model = models.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "def build_model():\n",
    "    return build_resnet(input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 50\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(X_test, y_test)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], 'b-', label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], 'r-', label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], 'b-', label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], 'r-', label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ecg/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "def build_cnn(input_shape, num_classes):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv1D(filters=64, kernel_size=7, activation='relu', input_shape=input_shape))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling1D(pool_size=2))\n",
    "    model.add(layers.Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling1D(pool_size=2))\n",
    "    model.add(layers.Conv1D(filters=256, kernel_size=3, activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling1D(pool_size=2))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(num_classes, activation='sigmoid'))\n",
    "\n",
    "    return model\n",
    "input_shape = X_train.shape[1:]\n",
    "num_classes = y_train.shape[1]\n",
    "model = build_cnn(input_shape, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(X_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer(input_shape, num_classes):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "\n",
    "    position_embedding = layers.Embedding(\n",
    "        input_dim=input_shape[0],\n",
    "        output_dim=input_shape[1]\n",
    "    )\n",
    "    positions = tf.range(start=0, limit=input_shape[0], delta=1)\n",
    "    positions = position_embedding(positions)\n",
    "    x = x + positions\n",
    "\n",
    "    num_heads = 4\n",
    "    ff_dim = 128\n",
    "    def transformer_encoder_block(inputs, head_size, ff_dim, dropout=0.1):\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "        x = layers.MultiHeadAttention(num_heads=num_heads, key_dim=head_size, dropout=dropout)(x, x)\n",
    "        x = layers.Dropout(dropout)(x)\n",
    "        x = layers.Add()([x, inputs])\n",
    "\n",
    "        x_ff = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        x_ff = layers.Dense(ff_dim, activation='relu')(x_ff)\n",
    "        x_ff = layers.Dropout(dropout)(x_ff)\n",
    "        x_ff = layers.Dense(inputs.shape[-1], activation='relu')(x_ff)\n",
    "        x_ff = layers.Dropout(dropout)(x_ff)\n",
    "        x = layers.Add()([x, x_ff])\n",
    "\n",
    "        return x\n",
    "    x = transformer_encoder_block(x, head_size=input_shape[1], ff_dim=ff_dim)\n",
    "    x = transformer_encoder_block(x, head_size=input_shape[1], ff_dim=ff_dim)\n",
    "    x = transformer_encoder_block(x, head_size=input_shape[1], ff_dim=ff_dim)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    outputs = layers.Dense(num_classes, activation='sigmoid')(x)\n",
    "    model = models.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "input_shape = X_train.shape[1:] \n",
    "num_classes = y_train.shape[1]\n",
    "\n",
    "model = build_transformer(input_shape, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 50\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(X_test, y_test)\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
